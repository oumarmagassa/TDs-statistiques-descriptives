{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0eb24c-7982-4583-b452-46b82fcadb1c",
   "metadata": {},
   "source": [
    "# <center>**Dask pour le traitement des données**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7c668-1d92-4b8f-8c32-1bfca0a1ece2",
   "metadata": {},
   "source": [
    "Le traitement des données, en particulier pour les grands jeux de données, nécessite des outils puissants. Dask et PySpark sont deux des outils les plus populaires de l'écosystème Python pour le traitement scalable des données. Dans ce notebook, nous allons explorer Dask, en vous fournissant des exemples pratiques et des réflexions sur quand et comment l'utiliser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7bbdc-c008-48bc-8365-685bc25e596f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d996dd4a-f7cb-45fb-9d39-e6b9148b9d1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Fichiers de données :** Aircraft_01.h5.zip, Aircraft_02.h5.zip, Aircraft_03.h5.zip, vol01.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5e09d-49fd-460d-896e-7d6108ba13ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Concepts de base**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80709da-768d-4120-a609-4ab6cf52826c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2302e9-2717-42a8-9649-2cf631b2fe42",
   "metadata": {},
   "source": [
    "**Qu'est-ce que Dask ?**\n",
    "\n",
    "Dask est une bibliothèque de calcul parallèle flexible pour l'analyse des données, permettant aux utilisateurs d'exploiter toute la puissance de leur CPU et de leur mémoire sans avoir besoin d'un cluster.\n",
    "\n",
    "**Caractéristiques principales :**\n",
    "\n",
    "- Planification dynamique des tâches.\n",
    "- S'intègre bien avec les bibliothèques Python populaires telles que Pandas et Numpy.\n",
    "- Permet d'utiliser Dask DataFrame, qui est similaire à Pandas mais fonctionne sur des ensembles de données plus grands que la mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a444e-a48a-4a2b-809a-70c677e2ef35",
   "metadata": {},
   "source": [
    "**Installer Dask :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caf3b2d5-477e-4299-aae3-11c135a7a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[complete] in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2025.11.0)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (8.3.0)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from dask[complete]) (25.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (6.0.3)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (1.1.0)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (22.0.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (4.4.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.1->dask[complete]) (0.4.6)\n",
      "Requirement already satisfied: locket in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (2.3.4)\n",
      "Requirement already satisfied: pandas>=2.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (2.3.3)\n",
      "Requirement already satisfied: distributed==2025.11.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (2025.11.0)\n",
      "Requirement already satisfied: bokeh>=3.1.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (3.8.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (3.1.6)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from distributed==2025.11.0->dask[complete]) (1.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from distributed==2025.11.0->dask[complete]) (7.1.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from distributed==2025.11.0->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from distributed==2025.11.0->dask[complete]) (3.2.2)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from distributed==2025.11.0->dask[complete]) (6.5.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from distributed==2025.11.0->dask[complete]) (2.5.0)\n",
      "Requirement already satisfied: zict>=3.0.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from distributed==2025.11.0->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.2 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (1.3.3)\n",
      "Requirement already satisfied: narwhals>=1.13 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (2.11.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (12.0.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (2025.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2>=2.10.3->dask[complete]) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.0->dask[complete]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[complete]) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dask[complete]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c16442-5328-4ac1-a862-3b659ffc24a1",
   "metadata": {},
   "source": [
    "**Importer Dask :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fba098ea-7680-4ea0-ab4d-18e2ae94233c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7564a-23bf-4e35-904d-79499f9b6525",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **2. Dask Client et Ordonnanceurs (Schedulers)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14efe1b4-25a9-4a1a-873c-e2ea3c4df7d4",
   "metadata": {},
   "source": [
    "#### **Dask Client :**\n",
    "Lorsque vous initialisez un `Client` sans arguments, cela met en place un cluster Dask local utilisant l'ordonnanceur distribué. Cela signifie que, même s'il est local (sur votre machine), il tire parti des avantages des capacités de calcul distribué de Dask. Voici deux scénarios différents d'utilisation de l'ordonnanceur distribué de Dask :\n",
    "   \n",
    "##### **Cluster Local (Machine unique) :**\n",
    "Dans ce contexte, \"distribué\" ne signifie pas que les tâches sont réparties entre différentes machines, mais plutôt que les tâches peuvent être distribuées sur tous les cœurs ou threads de votre machine unique. Cela permet des fonctionnalités de parallélisme avancées, des calculs asynchrones et donne accès à un tableau de bord web pour des informations diagnostiques. C'est ce qui se passe lorsque vous utilisez `Client()` sans vous connecter à un cluster externe.\n",
    "\n",
    "##### **Cluster Distribué (Plusieurs machines) :**\n",
    "Dask peut être configuré pour fonctionner sur un cluster de plusieurs machines, où les calculs sont vraiment distribués sur différents nœuds du cluster. Ceci est utile pour des jeux de données très volumineux ou des tâches nécessitant beaucoup de calculs, où les ressources d'une seule machine seraient insuffisantes.\n",
    "\n",
    "#### **Paramètres par défaut avec `Client()` :**\n",
    "- **Ouvriers (Workers)** : Équivalent au nombre de cœurs physiques sur votre machine.\n",
    "- **Threads par ouvrier** : 1 thread par défaut.\n",
    "- **Limite de mémoire** : Une fraction de votre mémoire totale disponible est utilisée pour chaque ouvrier.\n",
    "\n",
    "#### **Sans l'initialisation du Client :**\n",
    "Dask utilise par défaut l'ordonnanceur à base de threads pour les opérations. Il s'agit d'un ordonnanceur interne utilisant un pool de threads de taille fixe. Cela signifie que vous n'utilisez pas les capacités distribuées, mais exécutez plutôt des calculs en parallèle en utilisant des threads sur votre machine locale.\n",
    "\n",
    "#### **Ordonnanceurs :**\n",
    "Dask prend en charge divers ordonnanceurs (à base de threads, multiprocessing, distribué). Le choix de l'ordonnanceur détermine comment Dask exécute les calculs parallèles. En utilisant un `Client`, vous optez pour l'ordonnanceur distribué, même si tous les calculs sont effectués uniquement sur votre machine locale.\n",
    "\n",
    "En utilisant un `Client`, vous obtenez non seulement l'accès à l'ordonnanceur distribué, mais aussi à des outils supplémentaires comme le tableau de bord diagnostique de Dask, qui est utile pour surveiller et débuguer des calculs parallèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "226929e8-cb11-4e37-9389-544b92b3050a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\distributed\\node.py:188: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 56819 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous avez 8 coeurs disponibles.\n",
      "<Client: 'tcp://127.0.0.1:56822' processes=8 threads=8, memory=7.79 GiB>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dask.distributed import Client\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Vous avez {num_cores} coeurs disponibles.\")\n",
    "\n",
    "# Créer un client Dask avec autant de workers que de cœurs\n",
    "client = Client(n_workers=num_cores)\n",
    "\n",
    "# OU spécifier d'autres paramètres\n",
    "# client = Client(n_workers=num_cores, threads_per_worker=1, memory_limit=\"2GB\")\n",
    "\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc81ea-4b3e-458c-bdb3-f7f033177e5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **3. Usage basique**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a222a63-1306-4d19-b60c-6b877ed0e5e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Lire un fichier CSV :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e95502da-f72b-4802-b4e5-a096c5f4b1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv('vol01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e889a-31d3-404f-ae62-5a6d131c1adb",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Les opérations sont paresseuses (elles ne sont pas calculées immédiatement)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea01a220-9011-4e9e-94bb-27f320bea6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = df.groupby('ALT [ft]').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adbe2c6-4833-4d3e-8326-7372f6b4c5a3",
   "metadata": {},
   "source": [
    "**Calculer le résultat :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31438fd9-abde-4c0a-925a-6f92869e2bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EGT_1 [deg C]</th>\n",
       "      <th>EGT_2 [deg C]</th>\n",
       "      <th>FMV_1 [mm]</th>\n",
       "      <th>FMV_2 [mm]</th>\n",
       "      <th>HPTACC_1 [%]</th>\n",
       "      <th>HPTACC_2 [%]</th>\n",
       "      <th>M [Mach]</th>\n",
       "      <th>N1_1 [% rpm]</th>\n",
       "      <th>N1_2 [% rpm]</th>\n",
       "      <th>N2_1 [% rpm]</th>\n",
       "      <th>...</th>\n",
       "      <th>VIB_AN1_1 [mils]</th>\n",
       "      <th>VIB_AN1_2 [mils]</th>\n",
       "      <th>VIB_AN2_1 [ips]</th>\n",
       "      <th>VIB_AN2_2 [ips]</th>\n",
       "      <th>VIB_BN1_1 [mils]</th>\n",
       "      <th>VIB_BN1_2 [mils]</th>\n",
       "      <th>VIB_BN2_1 [ips]</th>\n",
       "      <th>VIB_BN2_2 [ips]</th>\n",
       "      <th>VSV_1 [mm]</th>\n",
       "      <th>VSV_2 [mm]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALT [ft]</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-23.183203</th>\n",
       "      <td>624.089720</td>\n",
       "      <td>646.255495</td>\n",
       "      <td>4.184419</td>\n",
       "      <td>4.167181</td>\n",
       "      <td>3.955259</td>\n",
       "      <td>34.638180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.638659</td>\n",
       "      <td>23.268110</td>\n",
       "      <td>54.129175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255179</td>\n",
       "      <td>0.072908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255179</td>\n",
       "      <td>54.841671</td>\n",
       "      <td>54.904192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-18.546562</th>\n",
       "      <td>522.915270</td>\n",
       "      <td>629.356460</td>\n",
       "      <td>3.721907</td>\n",
       "      <td>4.281759</td>\n",
       "      <td>13.576883</td>\n",
       "      <td>34.436766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.930621</td>\n",
       "      <td>23.371085</td>\n",
       "      <td>47.964212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063935</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215360</td>\n",
       "      <td>0.038137</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>0.016264</td>\n",
       "      <td>0.235550</td>\n",
       "      <td>54.722880</td>\n",
       "      <td>54.625251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-13.909922</th>\n",
       "      <td>455.030296</td>\n",
       "      <td>604.080006</td>\n",
       "      <td>3.340158</td>\n",
       "      <td>4.318302</td>\n",
       "      <td>18.761545</td>\n",
       "      <td>34.371200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.924909</td>\n",
       "      <td>23.482675</td>\n",
       "      <td>42.733648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025414</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>0.026664</td>\n",
       "      <td>0.180813</td>\n",
       "      <td>0.020831</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.018331</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>54.650832</td>\n",
       "      <td>54.309286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9.273281</th>\n",
       "      <td>459.165128</td>\n",
       "      <td>515.484695</td>\n",
       "      <td>3.519823</td>\n",
       "      <td>3.805518</td>\n",
       "      <td>18.757023</td>\n",
       "      <td>33.914843</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>20.859828</td>\n",
       "      <td>21.854212</td>\n",
       "      <td>43.861708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035070</td>\n",
       "      <td>0.009229</td>\n",
       "      <td>0.019842</td>\n",
       "      <td>0.157507</td>\n",
       "      <td>0.037223</td>\n",
       "      <td>0.017535</td>\n",
       "      <td>0.021534</td>\n",
       "      <td>0.259794</td>\n",
       "      <td>53.792965</td>\n",
       "      <td>53.728201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-4.636641</th>\n",
       "      <td>151.221978</td>\n",
       "      <td>167.811658</td>\n",
       "      <td>0.664343</td>\n",
       "      <td>0.761684</td>\n",
       "      <td>9.812895</td>\n",
       "      <td>34.463276</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>7.567465</td>\n",
       "      <td>7.771416</td>\n",
       "      <td>12.989454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030615</td>\n",
       "      <td>0.011362</td>\n",
       "      <td>0.003314</td>\n",
       "      <td>0.039768</td>\n",
       "      <td>0.045765</td>\n",
       "      <td>0.019884</td>\n",
       "      <td>0.007101</td>\n",
       "      <td>0.082061</td>\n",
       "      <td>54.843205</td>\n",
       "      <td>54.842708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39420.717939</th>\n",
       "      <td>673.867921</td>\n",
       "      <td>673.967144</td>\n",
       "      <td>9.430265</td>\n",
       "      <td>9.448990</td>\n",
       "      <td>89.403080</td>\n",
       "      <td>98.857785</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>101.819794</td>\n",
       "      <td>101.913563</td>\n",
       "      <td>80.391251</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464987</td>\n",
       "      <td>0.163126</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>2.405977</td>\n",
       "      <td>1.091528</td>\n",
       "      <td>0.079989</td>\n",
       "      <td>0.127983</td>\n",
       "      <td>9.043635</td>\n",
       "      <td>8.363996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39425.354580</th>\n",
       "      <td>677.533867</td>\n",
       "      <td>678.365084</td>\n",
       "      <td>9.499597</td>\n",
       "      <td>9.533730</td>\n",
       "      <td>89.571474</td>\n",
       "      <td>98.847483</td>\n",
       "      <td>0.637688</td>\n",
       "      <td>102.516471</td>\n",
       "      <td>102.669763</td>\n",
       "      <td>80.599093</td>\n",
       "      <td>...</td>\n",
       "      <td>1.534494</td>\n",
       "      <td>0.150374</td>\n",
       "      <td>0.043859</td>\n",
       "      <td>0.015949</td>\n",
       "      <td>2.507365</td>\n",
       "      <td>1.172230</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.118476</td>\n",
       "      <td>8.699101</td>\n",
       "      <td>8.066561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39429.991220</th>\n",
       "      <td>675.734082</td>\n",
       "      <td>676.188765</td>\n",
       "      <td>9.460387</td>\n",
       "      <td>9.497088</td>\n",
       "      <td>85.362855</td>\n",
       "      <td>98.886037</td>\n",
       "      <td>0.637105</td>\n",
       "      <td>102.185496</td>\n",
       "      <td>102.268745</td>\n",
       "      <td>80.431121</td>\n",
       "      <td>...</td>\n",
       "      <td>1.441343</td>\n",
       "      <td>0.151425</td>\n",
       "      <td>0.053279</td>\n",
       "      <td>0.014021</td>\n",
       "      <td>2.445236</td>\n",
       "      <td>1.144101</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.123383</td>\n",
       "      <td>8.617530</td>\n",
       "      <td>7.927390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39434.627861</th>\n",
       "      <td>683.831762</td>\n",
       "      <td>686.364994</td>\n",
       "      <td>9.602262</td>\n",
       "      <td>9.625997</td>\n",
       "      <td>98.830620</td>\n",
       "      <td>98.864523</td>\n",
       "      <td>0.640898</td>\n",
       "      <td>103.247826</td>\n",
       "      <td>103.299922</td>\n",
       "      <td>80.862150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.864372</td>\n",
       "      <td>0.114570</td>\n",
       "      <td>0.036454</td>\n",
       "      <td>0.036454</td>\n",
       "      <td>2.499717</td>\n",
       "      <td>1.156119</td>\n",
       "      <td>0.088532</td>\n",
       "      <td>0.130194</td>\n",
       "      <td>8.318321</td>\n",
       "      <td>7.797309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39439.264501</th>\n",
       "      <td>684.922459</td>\n",
       "      <td>687.262180</td>\n",
       "      <td>9.638285</td>\n",
       "      <td>9.635519</td>\n",
       "      <td>98.836977</td>\n",
       "      <td>98.851809</td>\n",
       "      <td>0.640898</td>\n",
       "      <td>103.419867</td>\n",
       "      <td>103.362520</td>\n",
       "      <td>80.897225</td>\n",
       "      <td>...</td>\n",
       "      <td>1.904732</td>\n",
       "      <td>0.100249</td>\n",
       "      <td>0.036454</td>\n",
       "      <td>0.036454</td>\n",
       "      <td>2.506226</td>\n",
       "      <td>1.175648</td>\n",
       "      <td>0.091135</td>\n",
       "      <td>0.127590</td>\n",
       "      <td>8.263242</td>\n",
       "      <td>7.742231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2142 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               EGT_1 [deg C]  EGT_2 [deg C]  FMV_1 [mm]  FMV_2 [mm]  \\\n",
       "ALT [ft]                                                              \n",
       "-23.183203        624.089720     646.255495    4.184419    4.167181   \n",
       "-18.546562        522.915270     629.356460    3.721907    4.281759   \n",
       "-13.909922        455.030296     604.080006    3.340158    4.318302   \n",
       "-9.273281         459.165128     515.484695    3.519823    3.805518   \n",
       "-4.636641         151.221978     167.811658    0.664343    0.761684   \n",
       "...                      ...            ...         ...         ...   \n",
       " 39420.717939     673.867921     673.967144    9.430265    9.448990   \n",
       " 39425.354580     677.533867     678.365084    9.499597    9.533730   \n",
       " 39429.991220     675.734082     676.188765    9.460387    9.497088   \n",
       " 39434.627861     683.831762     686.364994    9.602262    9.625997   \n",
       " 39439.264501     684.922459     687.262180    9.638285    9.635519   \n",
       "\n",
       "               HPTACC_1 [%]  HPTACC_2 [%]  M [Mach]  N1_1 [% rpm]  \\\n",
       "ALT [ft]                                                            \n",
       "-23.183203         3.955259     34.638180  0.000000     23.638659   \n",
       "-18.546562        13.576883     34.436766  0.000000     20.930621   \n",
       "-13.909922        18.761545     34.371200  0.000000     18.924909   \n",
       "-9.273281         18.757023     33.914843  0.000797     20.859828   \n",
       "-4.636641          9.812895     34.463276  0.000818      7.567465   \n",
       "...                     ...           ...       ...           ...   \n",
       " 39420.717939     89.403080     98.857785  0.637469    101.819794   \n",
       " 39425.354580     89.571474     98.847483  0.637688    102.516471   \n",
       " 39429.991220     85.362855     98.886037  0.637105    102.185496   \n",
       " 39434.627861     98.830620     98.864523  0.640898    103.247826   \n",
       " 39439.264501     98.836977     98.851809  0.640898    103.419867   \n",
       "\n",
       "               N1_2 [% rpm]  N2_1 [% rpm]  ...  VIB_AN1_1 [mils]  \\\n",
       "ALT [ft]                                   ...                     \n",
       "-23.183203        23.268110     54.129175  ...          0.072908   \n",
       "-18.546562        23.371085     47.964212  ...          0.063935   \n",
       "-13.909922        23.482675     42.733648  ...          0.025414   \n",
       "-9.273281         21.854212     43.861708  ...          0.035070   \n",
       "-4.636641          7.771416     12.989454  ...          0.030615   \n",
       "...                     ...           ...  ...               ...   \n",
       " 39420.717939    101.913563     80.391251  ...          1.464987   \n",
       " 39425.354580    102.669763     80.599093  ...          1.534494   \n",
       " 39429.991220    102.268745     80.431121  ...          1.441343   \n",
       " 39434.627861    103.299922     80.862150  ...          1.864372   \n",
       " 39439.264501    103.362520     80.897225  ...          1.904732   \n",
       "\n",
       "               VIB_AN1_2 [mils]  VIB_AN2_1 [ips]  VIB_AN2_2 [ips]  \\\n",
       "ALT [ft]                                                            \n",
       "-23.183203             0.000000         0.000000         0.255179   \n",
       "-18.546562             0.007852         0.000000         0.215360   \n",
       "-13.909922             0.004583         0.026664         0.180813   \n",
       "-9.273281              0.009229         0.019842         0.157507   \n",
       "-4.636641              0.011362         0.003314         0.039768   \n",
       "...                         ...              ...              ...   \n",
       " 39420.717939          0.163126         0.043011         0.014424   \n",
       " 39425.354580          0.150374         0.043859         0.015949   \n",
       " 39429.991220          0.151425         0.053279         0.014021   \n",
       " 39434.627861          0.114570         0.036454         0.036454   \n",
       " 39439.264501          0.100249         0.036454         0.036454   \n",
       "\n",
       "               VIB_BN1_1 [mils]  VIB_BN1_2 [mils]  VIB_BN2_1 [ips]  \\\n",
       "ALT [ft]                                                             \n",
       "-23.183203             0.072908          0.000000         0.000000   \n",
       "-18.546562             0.038137          0.007852         0.016264   \n",
       "-13.909922             0.020831          0.002916         0.018331   \n",
       "-9.273281              0.037223          0.017535         0.021534   \n",
       "-4.636641              0.045765          0.019884         0.007101   \n",
       "...                         ...               ...              ...   \n",
       " 39420.717939          2.405977          1.091528         0.079989   \n",
       " 39425.354580          2.507365          1.172230         0.084300   \n",
       " 39429.991220          2.445236          1.144101         0.067300   \n",
       " 39434.627861          2.499717          1.156119         0.088532   \n",
       " 39439.264501          2.506226          1.175648         0.091135   \n",
       "\n",
       "               VIB_BN2_2 [ips]  VSV_1 [mm]  VSV_2 [mm]  \n",
       "ALT [ft]                                                \n",
       "-23.183203            0.255179   54.841671   54.904192  \n",
       "-18.546562            0.235550   54.722880   54.625251  \n",
       "-13.909922            0.218100   54.650832   54.309286  \n",
       "-9.273281             0.259794   53.792965   53.728201  \n",
       "-4.636641             0.082061   54.843205   54.842708  \n",
       "...                        ...         ...         ...  \n",
       " 39420.717939         0.127983    9.043635    8.363996  \n",
       " 39425.354580         0.118476    8.699101    8.066561  \n",
       " 39429.991220         0.123383    8.617530    7.927390  \n",
       " 39434.627861         0.130194    8.318321    7.797309  \n",
       " 39439.264501         0.127590    8.263242    7.742231  \n",
       "\n",
       "[2142 rows x 54 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f095a-b1ca-45e1-9f95-b4e49eccb39d",
   "metadata": {},
   "source": [
    "**Filtrage des données :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd433db6-f271-42c6-ae09-b41f4e786e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_data = df[df['ALT [ft]'] > 1000].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae538c-2722-4e3a-a816-5cca456f3965",
   "metadata": {},
   "source": [
    "**Jointure des données :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8152d04-3391-4781-a7bf-c973a9435cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = df[df['ALT [ft]'] > 1000]\n",
    "df2 = df[df['ALT [ft]'] < 1500]\n",
    "joined_data = df1.merge(df2, on='ALT [ft]').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f557524-c4b3-4aea-9359-a86a36bba012",
   "metadata": {},
   "source": [
    "**Barre de progression avec le scheduler distribué :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25c4a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\oumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: colorama in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\oumar\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8015513c-df1c-4ca7-8989-a34976019687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c84e022f06d4167a8f1e6cad009de1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask.distributed import progress\n",
    "\n",
    "# Lire CSV\n",
    "df_dask = dd.read_csv('vol01.csv')\n",
    "\n",
    "# Filter, GroupBy and Calculer la moyenne (mean)\n",
    "result_dask = df_dask[df_dask['ALT [ft]'] > 1500].groupby('ALT [ft]')[\"M [Mach]\"].mean()\n",
    "\n",
    "# Calculer le résultat\n",
    "result_dask = result_dask.persist()\n",
    "\n",
    "progress(result_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c370161-c72c-49a4-80fd-031fb71372d2",
   "metadata": {},
   "source": [
    "**Barre de progression sur un scheduler à machine unique (Vous ne verrez pas la barre de progression si avez initialisez le Client) :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90b50f6b-a84c-42c8-aa60-e5bc550a82fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    # Lire CSV\n",
    "    df_dask = dd.read_csv('vol01.csv')\n",
    "\n",
    "    # Filter, GroupBy and Calculer la moyenne (mean)\n",
    "    result_dask = df_dask[df_dask['ALT [ft]'] > 1500].groupby('ALT [ft]')[\"M [Mach]\"].mean()\n",
    "\n",
    "    # Calculer le résultat\n",
    "    result_dask = result_dask.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3eb7e4-9f4f-426a-8fc3-d010f39cce0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **4. Dask Delayed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc2cae-9096-453c-89de-808e4164c3bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Qu'est-ce que Dask Delayed ?**\n",
    "\n",
    "Dask Delayed est un décorateur et une fonction. Il \"retarde\" l'exécution d'une fonction, au lieu de l'exécuter immédiatement. Lorsqu'une fonction est retardée, elle n'est pas encore calculée. Au lieu de cela, une représentation symbolique (ou tâche) du calcul est créée. Ces calculs symboliques peuvent ensuite être exécutés en parallèle.\n",
    "\n",
    "**Comment ça marche?**\n",
    "\n",
    "- Envelopper les calculs : En enveloppant les fonctions ou les calculs avec delayed, vous indiquez à Dask de ne pas les exécuter et de garder une trace des tâches pour un calcul parallèle ultérieur.\n",
    "\n",
    "- Construire des graphes de tâches : Dask construit en interne un graphique de tâches à partir de ces opérations différées. Un graphique de tâches est une représentation visuelle de la séquence et des dépendances des opérations.\n",
    "\n",
    "- Exécution parallèle : Une fois que votre calcul entier est construit en utilisant des fonctions retardées, vous pouvez calculer le résultat en parallèle en appelant la méthode `.compute()`.\n",
    "\n",
    "**Pourquoi utiliser Delayed ?**\n",
    "\n",
    "1. Algorithmes parallèles personnalisés : Bien que Dask fournisse des équivalents parallèles prêts à l'emploi pour de nombreuses opérations Python, Pandas et Numpy standard (comme Dask Array ou Dask DataFrame), vous pouvez rencontrer des scénarios dans lesquels vous avez besoin d'un parallélisme plus personnalisé. Dask Delayed vous permet de concevoir vos algorithmes parallèles sans avoir à vous plonger dans les complexités de la programmation parallèle.\n",
    "\n",
    "2. Flexibilité : Vous pouvez combiner des collections Dask (comme Dask Array ou DataFrame) avec des fonctions retardées, ce qui offre un environnement flexible pour construire des pipelines complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c9fb0-bcb0-49a4-9261-22e776d15dc7",
   "metadata": {},
   "source": [
    "**Voyons un exemple:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f19549f-e947-4c47-882e-d841e5411f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from dask import delayed\n",
    "\n",
    "# Quelques fonctions simples\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Sans delayed\n",
    "result = add(inc(1), inc(2))\n",
    "print(result)  # Output: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98bc9f-ba3d-43e7-ad4c-c761b84b0d96",
   "metadata": {},
   "source": [
    "**Maintenant, utilisons delayed:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd2f9a71-6813-4644-8757-bad6809b8754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Avec delayed\n",
    "inc_delayed = delayed(inc)\n",
    "add_delayed = delayed(add)\n",
    "\n",
    "# Ils ne calculent pas encore le résultat, mais construisent le graphe de calcul.\n",
    "a = inc_delayed(1)\n",
    "b = inc_delayed(2)\n",
    "result_delayed = add_delayed(a, b)\n",
    "\n",
    "# Calcul en parallèle (dans ce cas simple, il n'y a pas grand-chose à paralléliser)\n",
    "result = result_delayed.compute()\n",
    "print(result)  # Output: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cdf68-65c8-40ff-a669-2af0b1545e1a",
   "metadata": {},
   "source": [
    "**Lorsque vous retardez une fonction (delayed), elle ne calcule pas immédiatement son résultat. Au lieu de cela, elle garde une trace du calcul dans un graphe de tâches, que vous pouvez visualiser:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa493798-7a6d-4f6e-95bc-cbcbbd589773",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ac2ff-1c5d-4c87-aac7-ece943c573e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Exercice 1 : Extraction de fichiers Zip**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5113f8d-4756-469f-8687-61bc3f4b48be",
   "metadata": {},
   "source": [
    "### **Objectif :** \n",
    "Écrire une fonction qui extrait tous les fichiers ZIP d'un répertoire source vers un répertoire de destination.\n",
    "\n",
    "#### **Consignes :**\n",
    "\n",
    "- La fonction doit être nommée extraire_fichiers_zip.\n",
    "- La fonction doit accepter deux arguments :\n",
    "    - repertoire_source : Le répertoire dans lequel rechercher les fichiers ZIP.\n",
    "    - repertoire_destination : Le répertoire où les fichiers ZIP doivent être extraits.\n",
    "- Si le répertoire de destination n'existe pas, la fonction doit le créer.\n",
    "- Après l'extraction de chaque fichier ZIP, affichez un message informant l'utilisateur que le fichier a été extrait avec succès.\n",
    "\n",
    "#### **Bonus :**\n",
    "\n",
    "- Gérez les exceptions potentielles qui pourraient survenir lors de l'ouverture de fichiers ZIP corrompus.\n",
    "- Créez une fonction de logs pour enregistrer chaque action réalisée (par exemple, la création d'un répertoire, l'extraction réussie d'un fichier, etc.).\n",
    "\n",
    "#### **Astuces :**\n",
    "\n",
    "- Utilisez le module os pour interagir avec le système de fichiers.\n",
    "- Le module zipfile vous sera très utile pour gérer les fichiers ZIP.\n",
    "\n",
    "\n",
    "Bonne chance !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b36dcd8-a9ce-44bf-9b21-9281b37be48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile, os\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16336693-7e61-4a4e-9b10-4af743ebd19b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aircarts', 'TD5.ipynb', 'vol01.csv']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c904b36a-e179-4ca4-9a3e-54293bc2b5df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: 'data/data_zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m repertoire_destination = \u001b[33m'\u001b[39m\u001b[33mdata/data_extracted\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Print la liste des fichiers dans le répertoire source\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFichiers dans repertoire_source:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepertoire_source\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Appelez la fonction pour extraire tous les fichiers zip du source vers le répertoire de destination\u001b[39;00m\n\u001b[32m     25\u001b[39m extraire_fichiers_zip(repertoire_source, repertoire_destination)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] Le chemin d’accès spécifié est introuvable: 'data/data_zip'"
     ]
    }
   ],
   "source": [
    "def extraire_fichiers_zip(repertoire_source, repertoire_destination):\n",
    "    \"\"\"\n",
    "    Extrait tous les fichiers zip du répertoire source vers le répertoire de destination.\n",
    "\n",
    "    Paramètres:\n",
    "    - repertoire_source (str): Le répertoire dans lequel rechercher les fichiers zip.\n",
    "    - repertoire_destination (str): Le répertoire où les fichiers zip doivent être extraits.\n",
    "\n",
    "    Retourne:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "# Définir le répertoire où les fichiers zip sont stockés\n",
    "repertoire_source = 'data/data_zip'\n",
    "\n",
    "# Définir le répertoire où les fichiers doivent être extraits\n",
    "repertoire_destination = 'data/data_extracted'\n",
    "\n",
    "# Print la liste des fichiers dans le répertoire source\n",
    "print(\"Fichiers dans repertoire_source:\", os.listdir(repertoire_source))\n",
    "\n",
    "# Appelez la fonction pour extraire tous les fichiers zip du source vers le répertoire de destination\n",
    "extraire_fichiers_zip(repertoire_source, repertoire_destination)\n",
    "\n",
    "# Print la liste des fichiers dans le répertoire de destination après extraction\n",
    "print(\"Fichiers dans repertoire_destination après extraction:\", os.listdir(repertoire_destination))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4d5f6-b52f-44a6-bc99-0a681203c731",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Exercice 2 : Lecture d'un fichier HDF5 avec Dask**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a421c-4c8e-41d4-b8c2-57004738b7b3",
   "metadata": {},
   "source": [
    "Lisez le fichier HDF5 du premier avion \"Aircraft 01\" à l'aide de la fonction `read_hdf` de Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "313efb0e-a2ac-4dfc-9d0d-eb6721904761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "An error occurred while calling the read_hdf method registered to the pandas backend.\nOriginal Message: File(s) not found: data/data_extracted\\Aircraft_01.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\backends.py:140\u001b[39m, in \u001b[36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\dataframe\\io\\hdf.py:390\u001b[39m, in \u001b[36mread_hdf\u001b[39m\u001b[34m(pattern, key, start, stop, columns, chunksize, sorted_index, lock, mode)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile(s) not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n",
      "\u001b[31mOSError\u001b[39m: File(s) not found: data/data_extracted\\Aircraft_01.h5",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m nom_du_fichier = \u001b[33m'\u001b[39m\u001b[33mAircraft_01.h5\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m chemin_fichier = os.path.join(repertoire_destination, nom_du_fichier)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ddf = \u001b[43mdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchemin_fichier\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\backends.py:151\u001b[39m, in \u001b[36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: An error occurred while calling the read_hdf method registered to the pandas backend.\nOriginal Message: File(s) not found: data/data_extracted\\Aircraft_01.h5"
     ]
    }
   ],
   "source": [
    "nom_du_fichier = 'Aircraft_01.h5'\n",
    "chemin_fichier = os.path.join(repertoire_destination, nom_du_fichier)\n",
    "ddf = dd.read_hdf(chemin_fichier,'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0c8c4-fe48-404e-8fb5-2eaa167e4f10",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **L'Erreur**\n",
    "\n",
    "Lors de la tentative de lecture d'un fichier HDF5 stocké au format fixe à l'aide de la fonction `read_hdf` de Dask, vous pourriez rencontrer l'erreur ci-dessus.\n",
    "Cette erreur indique que le fichier HDF5 est dans un format qui ne lui permet pas d'être facilement partitionné pour un traitement parallèle par Dask.\n",
    "\n",
    "\n",
    "### **Pourquoi?**\n",
    "\n",
    "HDF5 est un format de stockage de données populaire capable de stocker de grands ensembles de données. Au sein du format HDF5, il existe deux principales façons de stocker des données :\n",
    "\n",
    "1. **Format Fixe** : C'est le format de stockage par défaut dans lequel les données sont stockées sous forme de bloc monolithique.\n",
    "2. **Format Tableau** : Dans ce format, les données sont stockées par morceaux, ce qui les rend essentiellement \"partitionnées\" et adaptées au traitement hors-mémoire et parallèle.\n",
    "\n",
    "Dask est une bibliothèque de calcul parallèle qui permet des opérations efficaces sur de grands ensembles de données en travaillant simultanément sur de petits morceaux ou partitions. Lors de la tentative de lecture d'un fichier HDF5 avec Dask, si les données sont stockées au format fixe, cela peut conduire à des inefficacités et des erreurs potentielles car le fichier n'est pas intrinsèquement partitionné.\n",
    "\n",
    "\n",
    "\n",
    "### **Objectif**\n",
    "\n",
    "- Votre tâche est d'écrire une fonction capable de gérer cette erreur. La fonction devrait :\n",
    "\n",
    "    1. Essayer de lire le fichier HDF5 à l'aide de Dask.\n",
    "    2. S'il rencontre l'erreur mentionnée ci-dessus, il devrait créer une version partitionnée du fichier au format `tableau` et la stocker dans un dossier \"data_dask\".\n",
    "    3. Une fois la version partitionnée créée, elle devrait lire cette version à l'aide de Dask et renvoyer le DataFrame Dask résultant.\n",
    "\n",
    "### **Bonus**\n",
    "\n",
    "- Améliorez la fonction pour vérifier d'abord si une version partitionnée du fichier existe déjà, et si c'est le cas, lisez cette version directement sans essayer de lire le fichier non partitionné.\n",
    "\n",
    "### **Astuce**\n",
    "\n",
    "- N'oubliez pas d'utiliser l'argument format='table' lors de la création d'une version partitionnée du fichier HDF5 avec pandas.\n",
    "\n",
    "Bonne chance !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86cd15a4-92c8-47e2-be93-7f9589247ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lire_hdf_dask(nom_fichier, repertoire='data/data_extracted'):\n",
    "    \"\"\"\n",
    "    Charge un fichier HDF5 dans un DataFrame Dask.\n",
    "    \n",
    "    Si le fichier HDF5 n'est pas partitionnable et qu'il n'y a pas déjà une version partitionnée,\n",
    "    une version partitionnée sera créée dans le répertoire 'data/data_dask'.\n",
    "    \n",
    "    Paramètres:\n",
    "    - nom_fichier (str): Nom du fichier HDF5.\n",
    "    - repertoire (str, facultatif): Répertoire contenant le fichier HDF5. Par défaut, 'data/data_extracted'.\n",
    "\n",
    "    Retour:def lire_hdf_dask(nom_fichier, repertoire='data/data_extracted'):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Chemins des fichiers\n",
    "    chemin_fichier_original = os.path.join(repertoire, nom_fichier)\n",
    "    repertoire_dask = 'data/data_dask'\n",
    "    nom_fichier_dask = nom_fichier.replace('.h5', '_dask.h5')\n",
    "    chemin_fichier_dask = os.path.join(repertoire_dask, nom_fichier_dask)\n",
    "    \n",
    "    # Vérifier si une version partitionnée existe déjà\n",
    "    if os.path.exists(chemin_fichier_dask):\n",
    "        print(f\"Version partitionnée trouvée : {chemin_fichier_dask}\")\n",
    "        print(\" Chargement de la version partitionnée...\")\n",
    "        ddf = dd.read_hdf(chemin_fichier_dask, '*')\n",
    "        print(\"Chargement réussi !\")\n",
    "        return ddf\n",
    "    \n",
    "    # Essayer de lire directement avec Dask\n",
    "    try:\n",
    "        print(f\"Tentative de lecture directe avec Dask : {chemin_fichier_original}\")\n",
    "        ddf = dd.read_hdf(chemin_fichier_original, '*')\n",
    "        print(\"Lecture directe réussie !\")\n",
    "        return ddf\n",
    "    \n",
    "    except ValueError as e:\n",
    "        if \"format='fixed'\" in str(e) or \"not partitionable\" in str(e):\n",
    "            print(f\"Le fichier n'est pas partitionnable (format 'fixed')\")\n",
    "            print(f\"Création d'une version partitionnée au format 'table'...\")\n",
    "            \n",
    "            # Créer le répertoire data_dask s'il n'existe pas\n",
    "            if not os.path.exists(repertoire_dask):\n",
    "                os.makedirs(repertoire_dask)\n",
    "                print(f\" Répertoire créé : {repertoire_dask}\")\n",
    "            \n",
    "            # Lire avec Pandas et obtenir les clés\n",
    "            with pd.HDFStore(chemin_fichier_original, mode='r') as store:\n",
    "                cles = store.keys()\n",
    "                print(f\"Nombre de vols trouvés : {len(cles)}\")\n",
    "            \n",
    "            # Créer un nouveau fichier HDF5 au format table\n",
    "            print(f\" Écriture du fichier partitionné : {chemin_fichier_dask}\")\n",
    "            with pd.HDFStore(chemin_fichier_dask, mode='w') as store_dask:\n",
    "                for i, cle in enumerate(cles):\n",
    "                    # Lire chaque vol avec Pandas\n",
    "                    df = pd.read_hdf(chemin_fichier_original, cle)\n",
    "                    \n",
    "                    # Écrire au format table (partitionnable)\n",
    "                    store_dask.put(cle, df, format='table', data_columns=True)\n",
    "                    \n",
    "                    # Afficher la progression\n",
    "                    if (i + 1) % 100 == 0 or (i + 1) == len(cles):\n",
    "                        print(f\"  Progression : {i + 1}/{len(cles)} vols traités\")\n",
    "            \n",
    "            print(\"Fichier partitionné créé avec succès !\")\n",
    "            \n",
    "            # Lire le nouveau fichier avec Dask\n",
    "            print(\"Chargement de la version partitionnée...\")\n",
    "            ddf = dd.read_hdf(chemin_fichier_dask, '*')\n",
    "            print(\"Chargement réussi !\")\n",
    "            return ddf\n",
    "        else:\n",
    "            # Si c'est une autre erreur, la relancer\n",
    "            raise e\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de la lecture du fichier : {str(e)}\")\n",
    "        raise e\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2232765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative de lecture directe avec Dask : data/data_extracted\\Aircraft_01.h5\n",
      " Erreur lors de la lecture du fichier : An error occurred while calling the read_hdf method registered to the pandas backend.\n",
      "Original Message: File(s) not found: data/data_extracted\\Aircraft_01.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "An error occurred while calling the read_hdf method registered to the pandas backend.\nOriginal Message: File(s) not found: data/data_extracted\\Aircraft_01.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\backends.py:140\u001b[39m, in \u001b[36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\dataframe\\io\\hdf.py:390\u001b[39m, in \u001b[36mread_hdf\u001b[39m\u001b[34m(pattern, key, start, stop, columns, chunksize, sorted_index, lock, mode)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile(s) not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n",
      "\u001b[31mOSError\u001b[39m: File(s) not found: data/data_extracted\\Aircraft_01.h5",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Testez la fonction\u001b[39;00m\n\u001b[32m      2\u001b[39m nom_du_fichier = \u001b[33m'\u001b[39m\u001b[33mAircraft_01.h5\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ddf = \u001b[43mlire_hdf_dask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnom_du_fichier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m DataFrame Dask chargé :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Nombre de partitions : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mddf.npartitions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mlire_hdf_dask\u001b[39m\u001b[34m(nom_fichier, repertoire)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Erreur lors de la lecture du fichier : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mlire_hdf_dask\u001b[39m\u001b[34m(nom_fichier, repertoire)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTentative de lecture directe avec Dask : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchemin_fichier_original\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     ddf = \u001b[43mdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchemin_fichier_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLecture directe réussie !\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ddf\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\backends.py:151\u001b[39m, in \u001b[36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: An error occurred while calling the read_hdf method registered to the pandas backend.\nOriginal Message: File(s) not found: data/data_extracted\\Aircraft_01.h5"
     ]
    }
   ],
   "source": [
    "# Testez la fonction\n",
    "nom_du_fichier = 'Aircraft_01.h5'\n",
    "ddf = lire_hdf_dask(nom_du_fichier)\n",
    "print(f\"\\n DataFrame Dask chargé :\")\n",
    "print(f\"  - Nombre de partitions : {ddf.npartitions}\")\n",
    "print(f\"  - Colonnes : {len(ddf.columns)}\")\n",
    "print(f\"  - Taille estimée : {ddf.memory_usage(deep=True).sum().compute() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e963b6-6372-4001-8f74-5f58891cac6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Exercice 3 : Exploration de Dask DataFrame**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b4b36-b9bb-4422-a224-a4788e5f3b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Dans cet exercice, vous allez explorer les caractéristiques et les propriétés de base d'un Dask DataFrame (ddf). L'objectif est de vous familiariser avec les composants clés et les méthodes associées à un Dask DataFrame.\n",
    "\n",
    "### **Objectifs :**\n",
    "\n",
    "1. **Nombre de Partitions** : \n",
    "   - Déterminez combien de partitions sont présentes dans le Dask DataFrame `ddf`.\n",
    "\n",
    "2. **Noms des Indices** : \n",
    "   - Utilisez la méthode `map_partitions` pour obtenir le nom des indices de chaque partition de `ddf`.\n",
    "   - Affichez les 10 premiers noms d'index.\n",
    "   - Comptez le nombre total de noms d'index que vous avez récupérés.\n",
    "\n",
    "3. **Colonnes du DataFrame** :\n",
    "   - Récupérez toutes les colonnes de `ddf` et stockez-les dans une variable.\n",
    "   - Comptez le nombre de colonnes.\n",
    "\n",
    "4. **Types de Données** :\n",
    "   - Examinez les types de données de chaque colonne dans `ddf`.\n",
    "\n",
    "5. **Aperçu d'une Partition** :\n",
    "   - Affichez les premières lignes de la première partition de `ddf`.\n",
    "\n",
    "### **Astuces :**\n",
    "\n",
    "- La méthode `map_partitions` peut être utilisée pour appliquer une fonction à chaque partition d'un Dask DataFrame.\n",
    "- La méthode `compute()` force l'évaluation d'une opération Dask. Souvenez-vous que Dask est paresseux par défaut, c'est-à-dire qu'il ne calcule pas les résultats tant que vous ne le lui demandez pas.\n",
    "- Vous pouvez accéder à une partition spécifique en utilisant `ddf.partitions[index]`.\n",
    "\n",
    "### **Bonus :**\n",
    "\n",
    "- Essayez d'afficher les dernières lignes de la dernière partition de `ddf`.\n",
    "- Explorez d'autres méthodes et propriétés associées à `ddf` pour obtenir une meilleure compréhension de vos données.\n",
    "\n",
    "Bonne chance !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f385c-a997-4a7c-b2b3-4da901e7aa2e",
   "metadata": {},
   "source": [
    "**1. Nombre de Partitions :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f90f1a-186f-495d-b3a9-bb5c02c6238f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative de lecture directe avec Dask : data/data_extracted\\Aircraft_01.h5\n",
      " Erreur lors de la lecture du fichier : An error occurred while calling the read_hdf method registered to the pandas backend.\n",
      "Original Message: File(s) not found: data/data_extracted\\Aircraft_01.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "An error occurred while calling the read_hdf method registered to the pandas backend.\nOriginal Message: File(s) not found: data/data_extracted\\Aircraft_01.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\backends.py:140\u001b[39m, in \u001b[36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\dataframe\\io\\hdf.py:390\u001b[39m, in \u001b[36mread_hdf\u001b[39m\u001b[34m(pattern, key, start, stop, columns, chunksize, sorted_index, lock, mode)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile(s) not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n",
      "\u001b[31mOSError\u001b[39m: File(s) not found: data/data_extracted\\Aircraft_01.h5",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Obtenir le nombre de partitions\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ddf = \u001b[43mlire_hdf_dask\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAircraft_01.h5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m   \n\u001b[32m      3\u001b[39m nombre_partitions = ddf.npartitions\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNombre de partitions dans ddf : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre_partitions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mlire_hdf_dask\u001b[39m\u001b[34m(nom_fichier, repertoire)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Erreur lors de la lecture du fichier : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mlire_hdf_dask\u001b[39m\u001b[34m(nom_fichier, repertoire)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTentative de lecture directe avec Dask : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchemin_fichier_original\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     ddf = \u001b[43mdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchemin_fichier_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLecture directe réussie !\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ddf\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\backends.py:151\u001b[39m, in \u001b[36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: An error occurred while calling the read_hdf method registered to the pandas backend.\nOriginal Message: File(s) not found: data/data_extracted\\Aircraft_01.h5"
     ]
    }
   ],
   "source": [
    "# Obtenir le nombre de partitions\n",
    "\n",
    "nombre_partitions = ddf.npartitions\n",
    "print(f\"Nombre de partitions dans ddf : {nombre_partitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf22aa4-444c-4b28-ac3f-baa911ec0f60",
   "metadata": {
    "tags": []
   },
   "source": [
    "**2. Noms des Indices :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b15c4536-6544-4649-9028-056c86bbe0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ddf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Obtenir le nom des indices de chaque partition\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m noms_index = \u001b[43mddf\u001b[49m.map_partitions(\u001b[38;5;28;01mlambda\u001b[39;00m df: df.index.name).compute()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Afficher les 10 premiers noms d'index\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Les 10 premiers noms d\u001b[39m\u001b[33m'\u001b[39m\u001b[33mindex :\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ddf' is not defined"
     ]
    }
   ],
   "source": [
    "# Obtenir le nom des indices de chaque partition\n",
    "noms_index = ddf.map_partitions(lambda df: df.index.name).compute()\n",
    "\n",
    "# Afficher les 10 premiers noms d'index\n",
    "print(\"\\n Les 10 premiers noms d'index :\")\n",
    "print(noms_index.head(10))\n",
    "\n",
    "# Compter le nombre total de noms d'index\n",
    "nombre_noms_index = len(noms_index)\n",
    "print(f\"\\n Nombre total de noms d'index récupérés : {nombre_noms_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc46544-e7a6-497d-b259-9154a37ac7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Si vous voulez voir les noms uniques\n",
    "noms_index_uniques = noms_index.unique()\n",
    "print(f\"\\n Noms d'index uniques : {noms_index_uniques}\")\n",
    "print(f\"   Nombre de noms uniques : {len(noms_index_uniques)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758868a-4fd4-4629-95d4-ccde559a5007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Récupérer toutes les colonnes\n",
    "colonnes = ddf.columns\n",
    "print(f\"\\n Colonnes du DataFrame :\")\n",
    "print(colonnes)\n",
    "\n",
    "# Compter le nombre de colonnes\n",
    "nombre_colonnes = len(colonnes)\n",
    "print(f\"\\n Nombre de colonnes : {nombre_colonnes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611a0c7-9710-4ca9-9989-e0698ba7b79d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**3. Colonnes du DataFrame :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cf37b-bb70-40d5-94ab-7582c347c691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Récupérer toutes les colonnes\n",
    "colonnes = ddf.columns\n",
    "print(f\"\\n Colonnes du DataFrame :\")\n",
    "print(colonnes)\n",
    "\n",
    "# Compter le nombre de colonnes\n",
    "nombre_colonnes = len(colonnes)\n",
    "print(f\"\\n Nombre de colonnes : {nombre_colonnes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05d445-9598-4806-af3d-1d566cbda838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Afficher les colonnes dans un format plus lisible\n",
    "print(\"\\n Liste des colonnes :\")\n",
    "for i, col in enumerate(colonnes, 1):\n",
    "    print(f\"  {i:3d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a7fb6-207c-4be9-bce8-9279911f0ffa",
   "metadata": {
    "tags": []
   },
   "source": [
    "**4. Types de Données :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9080820-a48d-473c-a537-5fc519fff8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examiner les types de données\n",
    "types_donnees = ddf.dtypes\n",
    "print(\"\\n Types de données de chaque colonne :\")\n",
    "print(types_donnees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4ba8c-a8de-43e6-bf71-5ffdad07ee30",
   "metadata": {
    "tags": []
   },
   "source": [
    "**5. Aperçu d'une Partition :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d049043b-5fa0-4ec9-8215-efc9a2b72d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Afficher les premières lignes de la première partition\n",
    "print(\"\\n  Aperçu de la première partition :\")\n",
    "premiere_partition = ddf.partitions[0].head()\n",
    "print(premiere_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fd4f2-dd67-436d-adae-6f6901141135",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Exercice 4 : Manipulation avancée avec Dask et comparaison avec Pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4390a-cee9-4d09-9bc7-95bf6fd0bd8e",
   "metadata": {},
   "source": [
    "Après avoir exploré les bases de `Dask DataFrame`, poursuivons avec des opérations plus avancées. L'objectif est de comprendre et de comparer le comportement de Dask par rapport à Pandas, notamment en termes de performances.\n",
    "\n",
    "### **Objectifs :**\n",
    "\n",
    "1. **Maximum de la colonne Vitesse** :\n",
    "   - Utilisez `map_partitions` pour trouver la vitesse maximale (`SPEED`) de chaque partition dans `ddf`.\n",
    "   - Calculez le temps d'exécution avec `%time`.\n",
    "\n",
    "2. **Extraction de la ligne avec la vitesse maximale** :\n",
    "   - Sélectionnez une partition spécifique de `ddf` (par exemple, la 676ème partition) et trouvez la ligne ayant la vitesse maximale.\n",
    "   - Utilisez une fonction personnalisée pour obtenir la ligne avec la vitesse maximale pour chaque partition.\n",
    "\n",
    "3. **Comparaison avec Pandas** :\n",
    "   - Lisez le fichier HDF5 du premier avion avec Pandas et obtenez la liste des clés.\n",
    "   - Comptez le nombre d'enregistrements et vérifiez les DataFrames vides.\n",
    "   - Supprimez les enregistrements vides.\n",
    "   - Mesurez le temps d'exécution pour trouver la ligne avec la vitesse maximale pour chaque DataFrame dans votre fichier avec Pandas.\n",
    "\n",
    "### **Astuces :**\n",
    "\n",
    "- La méthode `idxmax()` donne l'index de la première occurrence de la valeur maximale.\n",
    "- Vous pouvez mesurer le temps d'exécution d'une cellule dans un notebook Jupyter avec la commande magique `%time`.\n",
    "\n",
    "### **Bonus :**\n",
    "\n",
    "- Essayez d'optimiser vos fonctions pour réduire le temps d'exécution.\n",
    "- Comparez les résultats obtenus avec Dask et Pandas pour vérifier leur cohérence.\n",
    "- Réfléchissez à des scénarios où l'utilisation de Dask serait préférable à celle de Pandas et vice-versa.\n",
    "\n",
    "Bonne chance !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc36d5-7d43-48ab-b140-432221011d42",
   "metadata": {},
   "source": [
    "1. **Maximum de la colonne Vitesse :** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be705ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%time vitesses_max = ddf['SPEED'].map_partitions(lambda df: df.max()).compute()\n",
    "\n",
    "print(f\"Vitesses maximales par partition (10 premières) :\")\n",
    "print(vitesses_max.head(10))\n",
    "print(f\"\\nVitesse maximale globale : {vitesses_max.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb33b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques sur les vitesses maximales\n",
    "print(f\"\\nStatistiques des vitesses maximales par partition :\")\n",
    "print(f\"  Minimum : {vitesses_max.min():.2f}\")\n",
    "print(f\"  Maximum : {vitesses_max.max():.2f}\")\n",
    "print(f\"  Moyenne : {vitesses_max.mean():.2f}\")\n",
    "print(f\"  Médiane : {vitesses_max.median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e459d4f9-5970-4a24-907b-b62a5ddc7225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35144c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "157a35e6-39d3-4b5e-90f0-c23fc72986fd",
   "metadata": {},
   "source": [
    "2. **Extraction de la ligne avec la vitesse maximale :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34530f-6679-41d7-aca7-7ff925a95b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Pour une partition spécifique (par exemple partition 676)\n",
    "partition_index = 676\n",
    "if partition_index < ddf.npartitions:\n",
    "    partition_df = ddf.partitions[partition_index].compute()\n",
    "    ligne_max_speed = partition_df.loc[partition_df['SPEED'].idxmax()]\n",
    "    print(f\"Partition {partition_index} - Ligne avec vitesse maximale :\")\n",
    "    print(ligne_max_speed)\n",
    "else:\n",
    "    print(f\"  La partition {partition_index} n'existe pas (max: {ddf.npartitions-1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216fc6f-893b-4b38-a7d1-b040cd326e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa80cb-9428-4ae6-b857-3244ca617880",
   "metadata": {},
   "source": [
    "3. **Comparaison avec Pandas :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f02ce5-a9db-41e9-bec5-405551aee613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8cf40-42d8-47c0-85c0-e97ac6433f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Lecture avec Pandas\n",
    "import tables  # Nécessaire pour HDF5\n",
    "\n",
    "chemin_fichier = os.path.join('data/data_extracted', 'Aircraft_01.h5')\n",
    "\n",
    "# Obtenir les clés\n",
    "with pd.HDFStore(chemin_fichier, mode='r') as store:\n",
    "    cles = store.keys()\n",
    "    print(f\"Nombre total de vols (clés) : {len(cles)}\")\n",
    "\n",
    "# Compter les enregistrements et vérifier les DataFrames vides\n",
    "print(\"\\n Vérification des DataFrames...\")\n",
    "nombre_enregistrements = {}\n",
    "dataframes_vides = []\n",
    "\n",
    "for cle in cles:\n",
    "    df_temp = pd.read_hdf(chemin_fichier, cle)\n",
    "    nombre_enregistrements[cle] = len(df_temp)\n",
    "    if len(df_temp) == 0:\n",
    "        dataframes_vides.append(cle)\n",
    "\n",
    "print(f\"  Total d'enregistrements : {sum(nombre_enregistrements.values())}\")\n",
    "print(f\"  DataFrames vides : {len(dataframes_vides)}\")\n",
    "\n",
    "# Supprimer les enregistrements vides de la liste des clés\n",
    "cles_non_vides = [cle for cle in cles if cle not in dataframes_vides]\n",
    "print(f\"  DataFrames non vides : {len(cles_non_vides)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c34a4-b481-4d02-bfbe-e951f4a435c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b65ffc-0ff2-4bb2-9c61-a801c8f2af3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47f870-5369-4dcc-9d99-3977701ecaee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Il est obligatoire d'éliminer les vols qui n'ont pas de CEOD pour faire le calcul sans rencontrer d'erreurs (contrairement à DASK qui les éliminent automatiquement)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e799528-8aa7-4a2f-84ad-a91dd05f68bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e40ab7-7658-4ef3-8fb9-2680f5e5e7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################## Avec Tabata ########################\n",
    "#############################################################\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e873e-f825-4e9a-95ce-ffcd80e0a3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cba78ef-c67e-4b8c-9b75-6f1f8226653d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Exercice 5 : Analyse et Visualisation des Données avec Dask et Plotly**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194ccbe-fde8-42aa-a99a-640a97bb6c38",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons explorer davantage les fonctionnalités offertes par `Dask DataFrame` pour le traitement des données et nous plonger dans la visualisation à l'aide de `matplotlib` et `plotly`.\n",
    "\n",
    "### **Objectifs :**\n",
    "\n",
    "1. **Suppression des colonnes** :\n",
    "   - L'avion est équipé de deux moteurs. Supprimez les colonnes du moteur 2 à l'aide d'une liste prédéfinie.\n",
    "   - Vérifiez les colonnes restantes après suppression.\n",
    "\n",
    "2. **Visualisation de séries temporelles** :\n",
    "   - Définissez une fonction pour visualiser les séries temporelles de colonnes spécifiques d'une partition spécifique.\n",
    "   - Testez cette fonction en visualisant certaines séries temporelles.\n",
    "\n",
    "3. **Calcul de la longueur des vols** :\n",
    "   - Calculez la longueur de chaque vol (nombre d'enregistrements) dans `ddf`.\n",
    "   - Identifiez le vol ayant la durée maximale et minimale.\n",
    "\n",
    "4. **Visualisation de la distribution de la longueur des vols** :\n",
    "   - Utilisez `plotly` pour afficher la distribution de la longueur des vols à l'aide d'un graphique en boîte.\n",
    "\n",
    "### **Astuces :**\n",
    "\n",
    "- La méthode `drop` permet de supprimer des colonnes ou des lignes d'un DataFrame Dask.\n",
    "- Utilisez `map_partitions` pour appliquer des fonctions à chaque partition de `ddf`.\n",
    "- `plotly` est une bibliothèque de visualisation interactive qui peut être utilisée pour afficher une variété de types de graphiques.\n",
    "\n",
    "### **Bonus :**\n",
    "\n",
    "- Essayez d'autres types de visualisations avec `plotly` pour explorer davantage les données.\n",
    "- Réfléchissez à des techniques pour optimiser le traitement et la visualisation des données à l'aide de Dask.\n",
    "- Comparez les temps d'exécution entre différentes opérations pour mieux comprendre les avantages et inconvénients de Dask.\n",
    "\n",
    "Bonne chance !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6203bf-1f7a-46df-9635-76ecb0061188",
   "metadata": {},
   "source": [
    "**1. Suppression des colonnes :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb3355-9be2-4d7c-9654-b2f82bc960f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_var_e2 = [\n",
    "                   'FMV_2 [mm]', 'HPTACC_2 [%]', \n",
    "                   'N1_2 [% rpm]','N2_2 [% rpm]',\n",
    "                   'NAIV_2 [bool]', 'P0_2 [psia]',\n",
    "                   'PRV_2 [bool]', 'PS3_2 [psia]',\n",
    "                   'PT2_2 [mbar]', 'P_OIL_2 [psi]',\n",
    "                   'Q_2 [lb/h]', 'T1_2 [deg C]',\n",
    "                   'T2_2 [deg C]', 'T3_2 [deg C]',\n",
    "                   'T5_2 [deg C]', 'TBV_2 [%]',\n",
    "                   'TCASE_2 [deg C]', 'TLA_2 [deg]',\n",
    "                   'T_OIL_2 [deg C]', 'VBV_2 [mm]',\n",
    "                   'EGT_2 [deg C]','VIB_AN1_2 [mils]',\n",
    "                   'VIB_AN2_2 [ips]', 'VIB_BN1_2 [mils]',\n",
    "                   'VIB_BN2_2 [ips]', 'VSV_2 [mm]'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e23bc-8ec3-4603-8769-9c24923827c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512019a6-3ee0-4298-992c-de8b6a322dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848aee77-9e8d-4cba-946f-726c26505b03",
   "metadata": {},
   "source": [
    "**2. Visualisation de séries temporelles :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165b16a-3cc5-4834-be84-073c942ada06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def tracer_serie_temporelle(ddf, numero_partition, colonnes):\n",
    "    \"\"\"\n",
    "    Trace la série temporelle pour les colonnes spécifiées à partir d'une table donnée dans le fichier HDF5.\n",
    "    \n",
    "    Paramètres:\n",
    "    - ddf : dask dataframe\n",
    "    - numero_partition: Le numéro de la table à charger à partir du fichier HDF5.\n",
    "    - colonnes: Une liste de colonnes à tracer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Vérifier que la partition existe\n",
    "    if numero_partition >= ddf.npartitions:\n",
    "        print(f\" Erreur : La partition {numero_partition} n'existe pas (max: {ddf.npartitions-1})\")\n",
    "        return\n",
    "    \n",
    "    # Charger la partition\n",
    "    df_partition = ddf.partitions[numero_partition].compute()\n",
    "    \n",
    "    # Vérifier que les colonnes existent\n",
    "    colonnes_valides = [col for col in colonnes if col in df_partition.columns]\n",
    "    colonnes_invalides = [col for col in colonnes if col not in df_partition.columns]\n",
    "    \n",
    "    if colonnes_invalides:\n",
    "        print(f\" Colonnes non trouvées : {colonnes_invalides}\")\n",
    "    \n",
    "    if not colonnes_valides:\n",
    "        print(\"Aucune colonne valide à tracer\")\n",
    "        return\n",
    "    \n",
    "    # Créer le graphique\n",
    "    n_cols = len(colonnes_valides)\n",
    "    fig, axes = plt.subplots(n_cols, 1, figsize=(14, 3*n_cols))\n",
    "    \n",
    "    # Si une seule colonne, axes n'est pas une liste\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Nom de la partition (nom de l'index)\n",
    "    nom_partition = df_partition.index.name if df_partition.index.name else f\"Partition {numero_partition}\"\n",
    "    fig.suptitle(f'Séries temporelles - {nom_partition}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Tracer chaque colonne\n",
    "    for i, col in enumerate(colonnes_valides):\n",
    "        axes[i].plot(df_partition.index, df_partition[col], linewidth=0.8)\n",
    "        axes[i].set_ylabel(col, fontsize=10)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].set_xlabel('Index' if i == n_cols-1 else '')\n",
    "        \n",
    "        # Ajouter statistiques\n",
    "        mean_val = df_partition[col].mean()\n",
    "        max_val = df_partition[col].max()\n",
    "        min_val = df_partition[col].min()\n",
    "        axes[i].axhline(mean_val, color='r', linestyle='--', alpha=0.5, linewidth=1, label=f'Moyenne: {mean_val:.2f}')\n",
    "        axes[i].legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher des statistiques\n",
    "    print(f\"\\n Statistiques pour la partition {numero_partition} ({nom_partition}) :\")\n",
    "    print(f\"  Nombre de points : {len(df_partition)}\")\n",
    "    for col in colonnes_valides:\n",
    "        print(f\"\\n  {col} :\")\n",
    "        print(f\"    Min  : {df_partition[col].min():.2f}\")\n",
    "        print(f\"    Max  : {df_partition[col].max():.2f}\")\n",
    "        print(f\"    Moy  : {df_partition[col].mean():.2f}\")\n",
    "        print(f\"    Std  : {df_partition[col].std():.2f}\")\n",
    "\n",
    "\n",
    "# Tester la fonction\n",
    "print(\"\\nExemple de visualisation :\")\n",
    "tracer_serie_temporelle(ddf_e1, 0, ['ALT [ft]', 'EGT_1 [deg C]', 'FMV_1 [mm]', 'HPTACC_1 [%]', 'M [Mach]'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86364e6b-91d3-47c4-9db9-e143f64a2045",
   "metadata": {},
   "source": [
    "**3. Calcul de la longueur des vols :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c660d06-c31c-4cb8-8057-526f957ad74c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculer la longueur de chaque vol (partition)\n",
    "longueurs_vols = ddf_e1.map_partitions(len).compute()\n",
    "\n",
    "print(f\"Statistiques sur la longueur des vols :\")\n",
    "print(f\"  Nombre total de vols : {len(longueurs_vols)}\")\n",
    "print(f\"  Longueur minimale : {longueurs_vols.min()} enregistrements\")\n",
    "print(f\"  Longueur maximale : {longueurs_vols.max()} enregistrements\")\n",
    "print(f\"  Longueur moyenne : {longueurs_vols.mean():.2f} enregistrements\")\n",
    "print(f\"  Longueur médiane : {longueurs_vols.median():.2f} enregistrements\")\n",
    "\n",
    "# Identifier les vols avec durée max et min\n",
    "vol_avec_longueur_max = longueurs_vols.idxmax()\n",
    "vol_avec_longueur_min = longueurs_vols.idxmin()\n",
    "\n",
    "print(f\"\\n Vol le plus long :\")\n",
    "print(f\"  Partition : {vol_avec_longueur_max}\")\n",
    "print(f\"  Longueur : {longueurs_vols[vol_avec_longueur_max]} enregistrements\")\n",
    "\n",
    "print(f\"\\n Vol le plus court :\")\n",
    "print(f\"  Partition : {vol_avec_longueur_min}\")\n",
    "print(f\"  Longueur : {longueurs_vols[vol_avec_longueur_min]} enregistrements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34567f-00b6-4caf-9e9a-e94463f60b3e",
   "metadata": {},
   "source": [
    "4. **Visualisation de la distribution de la longueur des vols :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d89ef6-d567-4459-8f1f-2329cad3e370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Créer un DataFrame pour Plotly\n",
    "df_longueurs = pd.DataFrame({\n",
    "    'Vol': longueurs_vols.index,\n",
    "    'Longueur': longueurs_vols.values\n",
    "})\n",
    "\n",
    "# Box plot\n",
    "fig_box = px.box(\n",
    "    df_longueurs, \n",
    "    y='Longueur',\n",
    "    title='Distribution de la longueur des vols (Box Plot)',\n",
    "    labels={'Longueur': 'Nombre d\\'enregistrements'},\n",
    "    points='outliers'  # Afficher les outliers\n",
    ")\n",
    "\n",
    "fig_box.update_layout(\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    "    yaxis_title=\"Nombre d'enregistrements par vol\"\n",
    ")\n",
    "\n",
    "fig_box.show()\n",
    "\n",
    "# Histogramme\n",
    "fig_hist = px.histogram(\n",
    "    df_longueurs,\n",
    "    x='Longueur',\n",
    "    nbins=50,\n",
    "    title='Histogramme de la longueur des vols',\n",
    "    labels={'Longueur': 'Nombre d\\'enregistrements', 'count': 'Fréquence'}\n",
    ")\n",
    "\n",
    "fig_hist.update_layout(\n",
    "    height=400,\n",
    "    xaxis_title=\"Nombre d'enregistrements par vol\",\n",
    "    yaxis_title=\"Nombre de vols\"\n",
    ")\n",
    "\n",
    "fig_hist.show()\n",
    "\n",
    "# Violin plot (combine box plot et distribution)\n",
    "fig_violin = go.Figure()\n",
    "\n",
    "fig_violin.add_trace(go.Violin(\n",
    "    y=df_longueurs['Longueur'],\n",
    "    box_visible=True,\n",
    "    meanline_visible=True,\n",
    "    name='Distribution',\n",
    "    fillcolor='lightblue',\n",
    "    opacity=0.6\n",
    "))\n",
    "\n",
    "fig_violin.update_layout(\n",
    "    title='Distribution de la longueur des vols (Violin Plot)',\n",
    "    yaxis_title=\"Nombre d'enregistrements par vol\",\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig_violin.show()\n",
    "\n",
    "# Statistiques détaillées\n",
    "print(\"\\n Statistiques détaillées de la distribution :\")\n",
    "print(df_longueurs['Longueur'].describe())\n",
    "\n",
    "# Identifier les outliers\n",
    "Q1 = df_longueurs['Longueur'].quantile(0.25)\n",
    "Q3 = df_longueurs['Longueur'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "seuil_inf = Q1 - 1.5 * IQR\n",
    "seuil_sup = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df_longueurs[(df_longueurs['Longueur'] < seuil_inf) | (df_longueurs['Longueur'] > seuil_sup)]\n",
    "print(f\"\\n Outliers détectés : {len(outliers)} vols\")\n",
    "if len(outliers) > 0:\n",
    "    print(\"\\nVols outliers :\")\n",
    "    print(outliers.sort_values('Longueur', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af2f78-a074-4aba-8985-89592894113e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4f350-0a10-4641-8b95-8323960bc73b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
